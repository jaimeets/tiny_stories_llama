{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dacd71bf-49d6-4d36-8909-f03ff71c4256",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaimeet/workspace/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be7d9f4b-2e39-457b-8019-ed762d603a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaimeet/workspace/venv/lib/python3.10/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"roneneldan/TinyStories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "509e8428-998c-4334-ad92-abfb871b79e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset['train']\n",
    "validation = dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25afadb4-ad10-40ca-95a0-a012218e0385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "700a1085-9b6b-44d8-b525-210274331b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_str = \"\"\n",
    "for obj in train:\n",
    "    dataset_str += obj['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d594a1f-201a-4296-9a90-3f5b54e13a3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1899973203"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "179e98a3-e44c-4974-b74d-a18eb6b0bd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_len=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c359164-164d-49c6-8563-2e295edafd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(data, context):\n",
    "    X,Y = [],[]\n",
    "    for idx in range(0, len(data)-context, context):\n",
    "        X.append(data[idx:idx+context])\n",
    "        Y.append(data[idx+1:idx+1+context])\n",
    "    return torch.tensor(X), torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0fe6226-01c4-4dba-8dd1-7a994d58a17b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([73922, 32]), torch.Size([73922, 32]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = enc.encode(dataset_str[:10000000])\n",
    "X,Y = build_dataset(encoded, context_len)\n",
    "X.shape,Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e01a57ba-5987-4a78-9205-5a8d8081da67",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 1\n",
    "d_model = 128\n",
    "n_heads = 4\n",
    "context_len = context_len\n",
    "vocab_size = max(encoded)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "659900f3-62a6-4faa-bec3-ccc86234b874",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyLlama(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Blocks = nn.Sequential(*(Block() for _ in range(n_layers)))\n",
    "        self.Linear = nn.Linear(d_model, vocab_size)\n",
    "        self.Embedding = nn.Embedding(vocab_size, d_model, dtype=torch.float32)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.Embedding(x)\n",
    "        out = self.Blocks(x)\n",
    "        out  = self.Linear(out)\n",
    "        return out\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mha = MHA()\n",
    "        self.ffn = FFN()\n",
    "        self.ln1 = LayerNorm()\n",
    "        self.ln2 = LayerNorm()\n",
    "    def forward(self, x):\n",
    "        x = x + self.mha(self.ln1(x))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class MHA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sa = SelfAttention()\n",
    "        self.l1 = nn.Linear(d_model, d_model) \n",
    "    def forward(self, x):\n",
    "        #splits = torch.hsplit(x, n_heads)\n",
    "        concat = torch.cat([self.sa(x) for _ in range(n_heads)], dim=-1)\n",
    "        out = self.l1(concat)\n",
    "        return out\n",
    "\n",
    "        \n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.get_keys = nn.Linear(d_model, d_model // n_heads)\n",
    "        self.get_values = nn.Linear(d_model, d_model // n_heads)\n",
    "        self.get_queries = nn.Linear(d_model, d_model // n_heads)\n",
    "        self.rope = RotaryPositionalEmbeddings(d_model // n_heads)\n",
    "    def forward(self, x):\n",
    "        K = self.rope(self.get_keys(x))\n",
    "        V = self.get_values(x)\n",
    "        Q = self.rope(self.get_queries(x))\n",
    "        weightage = Q @ torch.transpose(K, -1, -2)\n",
    "        weightage = torch.tril(weightage)\n",
    "        weightage = weightage.masked_fill(weightage==0, float(\"-Inf\"))\n",
    "        scaled_weightage = weightage / (d_model // n_heads) ** 0.5\n",
    "        out = F.softmax(scaled_weightage, dim=-1) @ V\n",
    "        return out\n",
    "        \n",
    "        \n",
    "class FFN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(d_model, 4*d_model)\n",
    "        self.l2 = nn.Linear(2*d_model, d_model)\n",
    "        self.swiglu = SwiGLU()\n",
    "    def forward(self, x):\n",
    "        x = self.swiglu(self.l1(x))\n",
    "        x = self.l2(x)\n",
    "        return x\n",
    "        \n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-05\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        # self.beta = nn.Parameter(torch.zeros(d_model))  biases don't really help\n",
    "    def forward(self, x):\n",
    "        xmean = torch.mean(x, dim=-1, keepdims=True)\n",
    "        xvar = torch.var(x, dim=-1, keepdims=True)\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
    "        x = xhat * self.gamma\n",
    "        return x\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x, gate = x.chunk(2, dim=-1)\n",
    "        return F.silu(gate) * x\n",
    "\n",
    "class RotaryPositionalEmbeddings(nn.Module):\n",
    "    def __init__(self, d: int, base: int = 10_000):\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "        self.d = d\n",
    "        self.cos_cached = None\n",
    "        self.sin_cached = None\n",
    "\n",
    "    def _build_cache(self, x: torch.Tensor):\n",
    "        # if self.cos_cached is not None and x.shape[1] <= self.cos_cached.shape[0]:\n",
    "        #     return\n",
    "        seq_len = x.shape[1]\n",
    "        theta = 1. / (self.base ** (torch.arange(0, self.d, 2).float() / self.d)).to(x.device)\n",
    "        seq_idx = torch.arange(seq_len, device=x.device).float().to(x.device)\n",
    "        idx_theta = torch.einsum('n,d->nd', seq_idx, theta)\n",
    "        idx_theta2 = torch.cat([idx_theta, idx_theta], dim=1)\n",
    "        self.cos_cached = idx_theta2.cos()\n",
    "        self.sin_cached = idx_theta2.sin()\n",
    "\n",
    "    def _neg_half(self, x: torch.Tensor):\n",
    "        d_2 = self.d // 2\n",
    "        return torch.cat([-x[:, :, d_2:], x[:, :, :d_2]], dim=-1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        self._build_cache(x)\n",
    "        x_rope = x\n",
    "        neg_half_x = self._neg_half(x_rope)\n",
    "        x_rope = (x_rope * self.cos_cached) + (neg_half_x * self.sin_cached)\n",
    "        return x_rope "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "773622b6-e642-4aa3-b1d8-bb2e904f9c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8439405b-148d-45fb-b26b-b098fb5ae11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size):\n",
    "    len = X.shape[0]\n",
    "    idx = random.randint(0, len-batch_size)\n",
    "    return X[idx:idx+batch_size].to(device), Y[idx:idx+batch_size].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "853e2466-8206-40b9-8eff-56a3c4a5e864",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TinyLlama()\n",
    "model = model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "45f99c3c-a988-423d-a95a-e5cce5f61f5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25893631"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([torch.numel(p) for p in model.parameters() if p.requires_grad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c592753-8c18-4cee-8c2f-6b14c9679b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(n_steps):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    for i in range(n_steps):\n",
    "        inputs, labels = get_batch(32)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        outputs = torch.transpose(outputs, -2, -1)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999 or i==0:\n",
    "            if i==0:\n",
    "                last_loss = running_loss\n",
    "            else:\n",
    "                last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            running_loss = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "84c20d0f-6904-43f0-88f9-b45bd7c19b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 1 loss: 11.697783470153809\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 14\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(n_steps)\u001b[0m\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Gather data and report\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m999\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m i\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_one_epoch(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2179cf82-f737-4919-a13c-0d0197511507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d3e577b-0dc2-4255-92c4-6c0e1e7f7404",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(prompt, len):\n",
    "    input = torch.unsqueeze(torch.tensor(enc.encode(prompt), device=device), dim=0)\n",
    "    next_word_decoded = ''\n",
    "    idx=0\n",
    "    while idx!=len:\n",
    "        input_len = input.shape[-1]\n",
    "        output = model(input)\n",
    "        next_word_logits = output[0][input_len-1]\n",
    "        next_word_probs = F.softmax(next_word_logits, dim=0)\n",
    "        next_word = torch.multinomial(next_word_probs, 1)\n",
    "        next_word_decoded = enc.decode([next_word.item()])\n",
    "        next_word = next_word.view(1,1) # torch.tensor(124) -> torch.tensor([[124]]), makes it easy to concat\n",
    "        input = torch.cat((input, next_word), dim=-1)\n",
    "        input = torch.unsqueeze(input[0][-context_len:None], dim=0)\n",
    "        prompt += next_word_decoded\n",
    "        idx+=1\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d61ebf7c-4a9b-4ade-94ea-bf34974d3d6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"once upon a time there was dog named Bruno. But Bruno did not have many friends so he liked to look at her friends. Lily loved them cleaning. One day, Daisy was flying high and carrot. She gave her what she had to park.Anna and Ben were playing with her dad with her and opened it and felt its friends.Once upon a time, there was a little girl named Lily. Lily always was her favorite toy. She was always excited to be in the car. \\n\\nLily and Max was not scared!\\n\\nLily asked Lily, and Max, but Ben if they could go outside and play together they decided to play something meant best to sees the paintings very hard. She delighted, Spot, who was three years old and loved to swim in the park with his mom and dad. While they were, Lily and Max were friends. They were going to the park, because the greenIt was an lucky birthday, and she felt very special. She knew that some even made them theie playing with the carrot.\\n\\nThe sign was so that she had made choose a new butterfly. He quickly joined them by the bush to have found a big truck with named heavy toys and toys. One day, she went to play near her easy little flower with her mommy. One day, her mom made her a new way to call her mom. She was wait to eat. She could not do it again. She did not want to keep them safe.John and mom were very explained and how to find the same to have a lot of someone something is named the way to sit with his friend.Once upon a time, there was a little girl named Lily. She loved to see a big boat. One day, she saw a big green ball - a big red button. She wanted to sell it, but now it was too fast. She did not like the red ball and learned what it's.\\n\\nThe living room stopped and hurt. A deep boy is not a snack fun day at a squirrel to see. TheTweety was looking at her face and some fruits.\\n\\nWhile dinner, Lily's mom we were fun incredible up. They have a plan. Lily practiced cake a great time. They are full and happy. But building the red one just that animals was eating it.\\n\\nSue happy to have its tutor way to what to do.\\n\\nThey get to their until their they could different stared. Lily's mom saw that near the rabbit's moments that his truck was raining, but she didn't listen and she liked \\n\\nLily and Max feel the person were gone,\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"once upon a time there was dog named Bruno. But Bruno did not have many friends so he\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca51b42a-6a74-45d7-b1cf-bd258c22fc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './V1'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5149b83e-bcfe-428a-89e8-e6158908d7f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TinyLlama(\n",
       "  (Blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (mha): MHA(\n",
       "        (sa): SelfAttention(\n",
       "          (get_keys): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (get_values): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (get_queries): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (rope): RotaryPositionalEmbeddings()\n",
       "        )\n",
       "        (l1): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ffn): FFN(\n",
       "        (l1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (l2): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (swiglu): SwiGLU()\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "    )\n",
       "  )\n",
       "  (Linear): Linear(in_features=128, out_features=100255, bias=True)\n",
       "  (Embedding): Embedding(100255, 128)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TinyLlama()\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
