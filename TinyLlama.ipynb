{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dacd71bf-49d6-4d36-8909-f03ff71c4256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be7d9f4b-2e39-457b-8019-ed762d603a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaimeet/workspace/venv/lib/python3.10/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"roneneldan/TinyStories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "509e8428-998c-4334-ad92-abfb871b79e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset['train']\n",
    "validation = dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25afadb4-ad10-40ca-95a0-a012218e0385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "700a1085-9b6b-44d8-b525-210274331b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_str = \"\"\n",
    "for obj in train:\n",
    "    dataset_str += obj['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d594a1f-201a-4296-9a90-3f5b54e13a3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1899973203"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "179e98a3-e44c-4974-b74d-a18eb6b0bd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_len=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c359164-164d-49c6-8563-2e295edafd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(data, context):\n",
    "    X,Y = [],[]\n",
    "    for idx in range(0, len(data)-context, context):\n",
    "        X.append(data[idx:idx+context])\n",
    "        Y.append(data[idx+1:idx+1+context])\n",
    "    return torch.tensor(X), torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0fe6226-01c4-4dba-8dd1-7a994d58a17b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([73922, 32]), torch.Size([73922, 32]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = enc.encode(dataset_str[:10000000])\n",
    "X,Y = build_dataset(encoded, context_len)\n",
    "X.shape,Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e01a57ba-5987-4a78-9205-5a8d8081da67",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 1\n",
    "d_model = 128\n",
    "n_heads = 4\n",
    "context_len = context_len\n",
    "vocab_size = max(encoded)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "659900f3-62a6-4faa-bec3-ccc86234b874",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyLlama(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Blocks = nn.Sequential(*(Block() for _ in range(n_layers)))\n",
    "        self.Linear = nn.Linear(d_model, vocab_size)\n",
    "        self.Embedding = nn.Embedding(vocab_size, d_model, dtype=torch.float32)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.Embedding(x)\n",
    "        out = self.Blocks(x)\n",
    "        out  = self.Linear(out)\n",
    "        return out\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mha = MHA()\n",
    "        self.ffn = FFN()\n",
    "        self.ln1 = LayerNorm()\n",
    "        self.ln2 = LayerNorm()\n",
    "    def forward(self, x):\n",
    "        x = x + self.mha(self.ln1(x))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class MHA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sa = SelfAttention()\n",
    "        self.l1 = nn.Linear(d_model, d_model) \n",
    "    def forward(self, x):\n",
    "        #splits = torch.hsplit(x, n_heads)\n",
    "        concat = torch.cat([self.sa(x) for _ in range(n_heads)], dim=-1)\n",
    "        out = self.l1(concat)\n",
    "        return out\n",
    "\n",
    "        \n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.get_keys = nn.Linear(d_model, d_model // n_heads)\n",
    "        self.get_values = nn.Linear(d_model, d_model // n_heads)\n",
    "        self.get_queries = nn.Linear(d_model, d_model // n_heads)\n",
    "        self.rope = RotaryPositionalEmbeddings(d_model // n_heads)\n",
    "    def forward(self, x):\n",
    "        K = self.rope(self.get_keys(x))\n",
    "        V = self.get_values(x)\n",
    "        Q = self.rope(self.get_queries(x))\n",
    "        weightage = Q @ torch.transpose(K, -1, -2)\n",
    "        weightage = torch.tril(weightage)\n",
    "        weightage = weightage.masked_fill(weightage==0, float(\"-Inf\"))\n",
    "        scaled_weightage = weightage / (d_model // n_heads) ** 0.5\n",
    "        out = F.softmax(scaled_weightage, dim=-1) @ V\n",
    "        return out\n",
    "        \n",
    "        \n",
    "class FFN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(d_model, 4*d_model)\n",
    "        self.l2 = nn.Linear(2*d_model, d_model)\n",
    "        self.swiglu = SwiGLU()\n",
    "    def forward(self, x):\n",
    "        x = self.swiglu(self.l1(x))\n",
    "        x = self.l2(x)\n",
    "        return x\n",
    "        \n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-05\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        # self.beta = nn.Parameter(torch.zeros(d_model))  biases don't really help\n",
    "    def forward(self, x):\n",
    "        xmean = torch.mean(x, dim=-1, keepdims=True)\n",
    "        xvar = torch.var(x, dim=-1, keepdims=True)\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
    "        x = xhat * self.gamma\n",
    "        return x\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x, gate = x.chunk(2, dim=-1)\n",
    "        return F.silu(gate) * x\n",
    "\n",
    "class RotaryPositionalEmbeddings(nn.Module):\n",
    "    def __init__(self, d: int, base: int = 10_000):\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "        self.d = d\n",
    "        self.cos_cached = None\n",
    "        self.sin_cached = None\n",
    "\n",
    "    def _build_cache(self, x: torch.Tensor):\n",
    "        # if self.cos_cached is not None and x.shape[1] <= self.cos_cached.shape[0]:\n",
    "        #     return\n",
    "        seq_len = x.shape[1]\n",
    "        theta = 1. / (self.base ** (torch.arange(0, self.d, 2).float() / self.d)).to(x.device)\n",
    "        seq_idx = torch.arange(seq_len, device=x.device).float().to(x.device)\n",
    "        idx_theta = torch.einsum('n,d->nd', seq_idx, theta)\n",
    "        idx_theta2 = torch.cat([idx_theta, idx_theta], dim=1)\n",
    "        self.cos_cached = idx_theta2.cos()\n",
    "        self.sin_cached = idx_theta2.sin()\n",
    "\n",
    "    def _neg_half(self, x: torch.Tensor):\n",
    "        d_2 = self.d // 2\n",
    "        return torch.cat([-x[:, :, d_2:], x[:, :, :d_2]], dim=-1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        self._build_cache(x)\n",
    "        x_rope = x\n",
    "        neg_half_x = self._neg_half(x_rope)\n",
    "        x_rope = (x_rope * self.cos_cached) + (neg_half_x * self.sin_cached)\n",
    "        return x_rope "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "773622b6-e642-4aa3-b1d8-bb2e904f9c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8439405b-148d-45fb-b26b-b098fb5ae11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size):\n",
    "    len = X.shape[0]\n",
    "    idx = random.randint(0, len-batch_size)\n",
    "    return X[idx:idx+batch_size].to(device), Y[idx:idx+batch_size].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "853e2466-8206-40b9-8eff-56a3c4a5e864",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TinyLlama()\n",
    "model = model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45f99c3c-a988-423d-a95a-e5cce5f61f5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25893631"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([torch.numel(p) for p in model.parameters() if p.requires_grad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c592753-8c18-4cee-8c2f-6b14c9679b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(n_steps):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    for i in range(n_steps):\n",
    "        inputs, labels = get_batch(32)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        outputs = torch.transpose(outputs, -2, -1)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999 or i==0:\n",
    "            if i==0:\n",
    "                last_loss = running_loss\n",
    "            else:\n",
    "                last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            running_loss = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2179cf82-f737-4919-a13c-0d0197511507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TinyLlama(\n",
       "  (Blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (mha): MHA(\n",
       "        (sa): SelfAttention(\n",
       "          (get_keys): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (get_values): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (get_queries): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (rope): RotaryPositionalEmbeddings()\n",
       "        )\n",
       "        (l1): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ffn): FFN(\n",
       "        (l1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (l2): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (swiglu): SwiGLU()\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "    )\n",
       "  )\n",
       "  (Linear): Linear(in_features=128, out_features=100255, bias=True)\n",
       "  (Embedding): Embedding(100255, 128)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = './V1'\n",
    "model = TinyLlama()\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d3e577b-0dc2-4255-92c4-6c0e1e7f7404",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(prompt, len):\n",
    "    input = torch.unsqueeze(torch.tensor(enc.encode(prompt), device=device), dim=0)\n",
    "    next_word_decoded = ''\n",
    "    idx=0\n",
    "    while idx!=len:\n",
    "        input_len = input.shape[-1]\n",
    "        output = model(input)\n",
    "        next_word_logits = output[0][input_len-1]\n",
    "        next_word_probs = F.softmax(next_word_logits, dim=0)\n",
    "        next_word = torch.multinomial(next_word_probs, 1)\n",
    "        next_word_decoded = enc.decode([next_word.item()])\n",
    "        next_word = next_word.view(1,1) # torch.tensor(124) -> torch.tensor([[124]]), makes it easy to concat\n",
    "        input = torch.cat((input, next_word), dim=-1)\n",
    "        input = torch.unsqueeze(input[0][-context_len:None], dim=0)\n",
    "        prompt += next_word_decoded\n",
    "        idx+=1\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d61ebf7c-4a9b-4ade-94ea-bf34974d3d6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'once upon a time there was dog named Bruno. But Bruno did not have many friends so he was safe in the done. They had learned a valuable lesson. She turned all about her toys and bigger than the little girl. They knew that they liked their new surprise and clothes.Lily and Ben were just friends. They liked to play and run and slide. They liked to run and play on the swings and slide magical next to see the grass. They hoped to do they did not know that Mommy had an idea. They played with them. They strange circles and smile. They had a lot of fun, hours walking on and across the woods on the sea. They saw a big tree with their mom. They were playing his fields and played with the pulled home with him. Timmy felt like so much fun with each other. From that day on, Timmy always smiled, and promised to go to a special until the stopped calling him for lunch. They were busy! They felt embarrassed and p: \"I think!\"\\n\\nJack ran to each other and cover and can bit of them!\" Lily agreed and opened it in and called out, \"Now the mouse. I am glad you were Sally. You could hug out of the crying.Their tell them to go home with Mom and Dad. She comes to the kitchen for dinner. She turn around and play at her house,le, taking into theYes, Sally. \\n\\n\"Go away, we escaped otherHelp\".\\n\\nLily was sad and decided to play and play near the even sail on the came back and led to the playground. They had put them in a big red duck come and draw up with them and tell their dad.\\n\\n\"Yes, Ben,\" Lily said. \"The lady\". Lily and Max ate,uggled and went inside for a long time and the spirit explained.\\n\\n\"Don\\'t touch the other paw your toys, but I can help you make a flower even buy a very much better imagination,\" Jack said.\\n\\nSuddenly the sun came out of the garden. \"No, Ben, I will The door,\" Tim shouted back and the big little girl. \"Stopuch!\" Tim smiled and said, \"Thank you.\"\\n\\nTimmy!\" Lily stops smiles and sees high! From that day on, Jill. She does not like that. She chewed in her hand andty was so proud of what they wereFrom that agreed to show the people. She would always keep going.\\n\\nOne day, the our princess food. She asked her mom and dad to make anyone who.Lily and Ben and Lily were playing with'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"once upon a time there was dog named Bruno. But Bruno did not have many friends so he\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca51b42a-6a74-45d7-b1cf-bd258c22fc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './V1'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
